{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgSize = 300\n",
    "folder_data = r'D:\\Vietname_sig\\Data'\n",
    "label_to_int = {'Hello': 0, 'Thanks': 1, 'ByeBye': 2, 'SeeYouAgain': 3, 'Sorry': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for foldername in os.listdir(folder_data):\n",
    "    if not foldername.startswith('.'):\n",
    "        folder_path = os.path.join(folder_data, foldername)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if not filename.startswith('.'):\n",
    "                img = cv.imread(os.path.join(folder_path, filename))\n",
    "                img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "                img = cv.resize(img, (imgSize, imgSize))\n",
    "                data.append(img)\n",
    "                labels.append(foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype=\"float32\") / 255.0\n",
    "labels = [label_to_int[label] for label in labels]\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, tempX, trainY, tempY = train_test_split(data, labels, test_size=0.2, random_state=2)\n",
    "valX, testX, valY, testY = train_test_split(tempX, tempY, test_size=0.5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(imgSize, imgSize, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 298, 298, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 149, 149, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 147, 147, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 73, 73, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 71, 71, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 35, 35, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 156800)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               40141056  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,235,013\n",
      "Trainable params: 40,235,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(trainX.reshape(-1, imgSize, imgSize, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "26/26 [==============================] - 9s 191ms/step - loss: 2.1718 - accuracy: 0.2430 - val_loss: 1.5658 - val_accuracy: 0.3585\n",
      "Epoch 2/25\n",
      "26/26 [==============================] - 4s 133ms/step - loss: 1.5428 - accuracy: 0.3028 - val_loss: 1.2061 - val_accuracy: 0.5472\n",
      "Epoch 3/25\n",
      "26/26 [==============================] - 4s 130ms/step - loss: 1.4157 - accuracy: 0.3980 - val_loss: 1.1105 - val_accuracy: 0.5283\n",
      "Epoch 4/25\n",
      "26/26 [==============================] - 4s 131ms/step - loss: 1.2181 - accuracy: 0.4933 - val_loss: 0.8714 - val_accuracy: 0.7925\n",
      "Epoch 5/25\n",
      "26/26 [==============================] - 4s 132ms/step - loss: 1.0558 - accuracy: 0.5922 - val_loss: 0.5003 - val_accuracy: 0.9434\n",
      "Epoch 6/25\n",
      "26/26 [==============================] - 4s 134ms/step - loss: 0.9860 - accuracy: 0.6264 - val_loss: 0.3591 - val_accuracy: 0.9434\n",
      "Epoch 7/25\n",
      "26/26 [==============================] - 4s 132ms/step - loss: 0.8084 - accuracy: 0.6984 - val_loss: 0.2367 - val_accuracy: 0.9717\n",
      "Epoch 8/25\n",
      "26/26 [==============================] - 4s 136ms/step - loss: 0.7133 - accuracy: 0.7296 - val_loss: 0.2233 - val_accuracy: 0.9528\n",
      "Epoch 9/25\n",
      "26/26 [==============================] - 3s 130ms/step - loss: 0.7163 - accuracy: 0.7460 - val_loss: 0.1108 - val_accuracy: 0.9811\n",
      "Epoch 10/25\n",
      "26/26 [==============================] - 4s 133ms/step - loss: 0.6025 - accuracy: 0.7729 - val_loss: 0.0782 - val_accuracy: 1.0000\n",
      "Epoch 11/25\n",
      "26/26 [==============================] - 4s 140ms/step - loss: 0.5446 - accuracy: 0.7924 - val_loss: 0.0664 - val_accuracy: 0.9811\n",
      "Epoch 12/25\n",
      "26/26 [==============================] - 4s 137ms/step - loss: 0.4823 - accuracy: 0.8217 - val_loss: 0.0930 - val_accuracy: 0.9717\n",
      "Epoch 13/25\n",
      "26/26 [==============================] - 3s 127ms/step - loss: 0.4511 - accuracy: 0.8327 - val_loss: 0.0684 - val_accuracy: 0.9906\n",
      "Epoch 14/25\n",
      "26/26 [==============================] - 3s 123ms/step - loss: 0.4519 - accuracy: 0.8376 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "26/26 [==============================] - 3s 125ms/step - loss: 0.3849 - accuracy: 0.8718 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "26/26 [==============================] - 3s 126ms/step - loss: 0.3581 - accuracy: 0.8840 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "26/26 [==============================] - 3s 124ms/step - loss: 0.2957 - accuracy: 0.8950 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "26/26 [==============================] - 3s 126ms/step - loss: 0.2800 - accuracy: 0.8950 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "26/26 [==============================] - 3s 124ms/step - loss: 0.1949 - accuracy: 0.9377 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "26/26 [==============================] - 3s 124ms/step - loss: 0.1885 - accuracy: 0.9365 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "26/26 [==============================] - 3s 124ms/step - loss: 0.2206 - accuracy: 0.9182 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "26/26 [==============================] - 3s 123ms/step - loss: 0.1814 - accuracy: 0.9414 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "26/26 [==============================] - 3s 126ms/step - loss: 0.1992 - accuracy: 0.9423 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "26/26 [==============================] - 3s 129ms/step - loss: 0.1765 - accuracy: 0.9402 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "26/26 [==============================] - 4s 137ms/step - loss: 0.1289 - accuracy: 0.9670 - val_loss: 0.0065 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d564941b40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(trainX.reshape(-1, imgSize, imgSize, 1), trainY, batch_size=32),\n",
    "          validation_data=(valX.reshape(-1, imgSize, imgSize, 1), valY),\n",
    "          epochs= 25, steps_per_epoch=len(trainX) // 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
